{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a88d62a-4c39-41c4-bad0-a63d87ed24a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nadda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\nadda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a35106-06a1-4812-93c6-b2dd3a957033",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "042aa8c7-5820-4e20-9f28-a2f4846c0a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'How', 'are', 'you', 'doing', '?', 'Today', 'is', 'a', 'sunny', 'day', '.']\n"
     ]
    }
   ],
   "source": [
    "# Word Tokenization\n",
    "text = 'Hello everyone. How are you doing?  Today is a sunny day.'\n",
    "word_tokens = word_tokenize(text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65eb11a9-352c-4794-9584-041ee894af2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello everyone.', 'How are you doing?', 'Today is a sunny day.']\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text = 'Hello everyone. How are you doing?  Today is a sunny day.'\n",
    "sent_tokens = sent_tokenize(text)\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e17cc3-3c6c-4396-a623-e8b6bde26687",
   "metadata": {},
   "source": [
    "### Tokenization + Punctuation Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a08d189-cb1e-459d-8dc5-76c803bb5d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', 'How', 'are', 'you', 'doing', 'Today', 'is', 'a', 'sunny', 'day']\n"
     ]
    }
   ],
   "source": [
    "# RegexpTokenizer \n",
    "from nltk.tokenize import RegexpTokenizer  \n",
    "regexp = RegexpTokenizer(r'\\w+')\n",
    "text = 'Hello everyone. How are you doing?  Today is a sunny day.'\n",
    "word_tokens = regexp.tokenize(text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bb09912-734d-43f9-99f1-078601d45830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'everyone',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " 'Today',\n",
       " 'is',\n",
       " 'a',\n",
       " 'sunny',\n",
       " 'day']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use regular expression\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize \n",
    "text = 'Hello everyone. How are you doing?  Today is a sunny day.'\n",
    "word_tokens = word_tokenize(text)\n",
    "# \\w = word characters in a string, \\s = white space\n",
    "word_tokens = [re.sub(r'[^\\w\\s]', '', token) for token in word_tokens if re.sub(r'[^\\w\\s]', '', token)]  \n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e211ad-7e3c-4059-a0ca-18b4fd0bb24f",
   "metadata": {},
   "source": [
    "### Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "791a7b0f-0d85-478e-9f0d-ab7e3c09982e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nadda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "corpus_stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a1b8c0a-0bae-4bed-aef7-8decc51a0903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weren', 'over', 'should', 'needn', 'same', \"she'll\", 'doing', 'under', 'were', 'itself', 'again', \"we've\", \"mightn't\", 'mustn', 'our', 's', 'doesn', 'until', \"you've\", 'him', 'do', 'the', \"you'll\", 'above', 'not', 'if', 'these', 'mightn', 'because', 'nor', 'you', \"hasn't\", \"they'll\", 'am', \"it's\", \"they'd\", 'yourselves', 'just', 'o', 'their', 'during', \"he's\", 'into', \"shouldn't\", 'wouldn', 'shan', 'theirs', 'wasn', 'll', \"we'd\", 'd', 'her', 'each', 'where', 'no', 'was', 'be', 'why', \"i've\", 'in', 'can', \"they've\", 'are', 'up', 'as', 'own', 'will', \"should've\", \"you'd\", \"haven't\", 'such', \"we're\", 'hers', 'y', \"we'll\", 'all', 'too', 'this', 'didn', 'me', \"that'll\", 'when', 'hasn', 're', 'have', \"hadn't\", 'has', 'hadn', 'more', 'further', 'some', \"he'll\", 'only', 'they', 'yourself', 'between', 'an', 'with', 'does', 'ours', 'a', \"she'd\", \"he'd\", \"i'm\", 'those', 'but', 'of', \"wouldn't\", 'm', 'don', \"isn't\", 'aren', 'very', \"it'd\", \"shan't\", 'it', 'to', \"needn't\", 've', \"they're\", \"she's\", 'both', 'other', 'from', 'your', \"it'll\", 'which', \"i'll\", 'himself', 'most', 'once', 'then', 'myself', 'she', 'by', 'my', 'having', 'ourselves', \"won't\", 'ain', 'been', 'below', 'or', 'about', \"don't\", \"i'd\", 'ma', 'them', 'while', 'couldn', 'any', 'is', \"couldn't\", 'we', 'yours', 'he', 'shouldn', 'at', 'few', 'now', 'that', 'how', 'his', 'against', 'so', 'whom', 'on', 'had', 'through', 'did', 'what', 'isn', 'off', \"aren't\", 'being', \"weren't\", \"wasn't\", 'haven', 'than', \"you're\", 'there', 'herself', 't', 'themselves', 'and', 'i', \"doesn't\", 'before', 'for', 'out', 'down', \"didn't\", 'who', \"mustn't\", 'after', 'its', 'won', 'here'}\n"
     ]
    }
   ],
   "source": [
    "print(corpus_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8be80e3-3fc5-4298-92c4-f76d4204b2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['Hello', 'everyone', 'How', 'are', 'you', 'doing', 'Today', 'is', 'a', 'sunny', 'day']\n",
      "After stop-word removal: ['Hello', 'everyone', 'Today', 'sunny', 'day']\n"
     ]
    }
   ],
   "source": [
    "filtered_word_tokens = [word for word in word_tokens if not word.lower() in corpus_stop_words]\n",
    "print(\"Original words:\", word_tokens)\n",
    "print(\"After stop-word removal:\", filtered_word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41abbedf-550f-4544-b2d4-366ae6d290b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = filtered_word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0d1d53-d8dd-476c-8467-4a524bad614b",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "116bd78b-c9a4-4a25-8b09-537b0e6d5505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db538480-9ebc-4e1d-87b7-4914efca8fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['changed', 'changing', 'changable']\n",
      "Stemmed words: ['chang', 'chang', 'changabl']\n"
     ]
    }
   ],
   "source": [
    "# Stemming example\n",
    "word_list = ['changed', 'changing', 'changable']\n",
    "stemmed_word_list = [porter_stemmer.stem(word) for word in word_list]\n",
    " \n",
    "print(\"Original words:\", word_list)\n",
    "print(\"Stemmed words:\", stemmed_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "811c732c-142f-4450-b794-3078ebded659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['walking', 'walked', 'walker']\n",
      "Stemmed words: ['walk', 'walk', 'walker']\n"
     ]
    }
   ],
   "source": [
    "# Stemming example\n",
    "word_list = ['walking', 'walked', 'walker']\n",
    "stemmed_word_list = [porter_stemmer.stem(word) for word in word_list]\n",
    "\n",
    "print(\"Original words:\", word_list)\n",
    "print(\"Stemmed words:\", stemmed_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffa8bf2f-0d62-4291-959e-a8791e9c0100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['sleeping', 'sleeper', 'slept']\n",
      "Stemmed words: ['sleep', 'sleeper', 'slept']\n"
     ]
    }
   ],
   "source": [
    "# Stemming example\n",
    "word_list = ['sleeping', 'sleeper', 'slept']\n",
    "stemmed_word_list = [porter_stemmer.stem(word) for word in word_list]\n",
    " \n",
    "print(\"Original words:\", word_list)\n",
    "print(\"Stemmed words:\", stemmed_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32c8abc6-8bc2-4108-a086-3de8ef1da39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['better', 'happily', 'happy']\n",
      "Stemmed words: ['better', 'happili', 'happi']\n"
     ]
    }
   ],
   "source": [
    "# Stemming example\n",
    "word_list = ['better', 'happily', 'happy']\n",
    "stemmed_word_list = [porter_stemmer.stem(word) for word in word_list]\n",
    " \n",
    "print(\"Original words:\", word_list)\n",
    "print(\"Stemmed words:\", stemmed_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdf0bbc2-746c-4cc6-9caa-307ace7411ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['Hello', 'everyone', 'Today', 'sunny', 'day']\n",
      "Stemmed words: ['hello', 'everyon', 'today', 'sunni', 'day']\n"
     ]
    }
   ],
   "source": [
    "# Stemming example\n",
    "word_list = word_tokens\n",
    "stemmed_word_list = [porter_stemmer.stem(word) for word in word_list]\n",
    " \n",
    "print(\"Original words:\", word_list)\n",
    "print(\"Stemmed words:\", stemmed_word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d88db7-44d5-402c-9f8d-2f0d3e111ab1",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a9322e0-6f11-4b54-b719-3caf07390ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nadda\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Lemmatization\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb2d8944-f84c-4847-888d-6f980cb827e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2859fce6-915b-465c-bdd1-c5d465de9099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['changing', 'changed', 'walking', 'walked', 'sleeping', 'slept']\n",
      "Lemmas: ['change', 'change', 'walk', 'walk', 'sleep', 'sleep']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization example\n",
    "# 'n' for nouns, 'v' for verbs, 'a' for adjectives, 'r' for adverbs, 's' for satellite (auxilliary) adjectives.\n",
    "word_list = ['changing', 'changed', 'walking', 'walked', 'sleeping', 'slept']\n",
    "lemma_list = [lemmatizer.lemmatize(word,pos='v') for word in word_list]\n",
    " \n",
    "print(\"Original words:\", word_list)\n",
    "print(\"Lemmas:\", lemma_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33059965-30dd-4313-9e38-3dacdd672a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['runner', 'walker', 'sleeper']\n",
      "Lemmas: ['runner', 'walker', 'sleeper']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization example\n",
    "word_list = ['runner', 'walker', 'sleeper']\n",
    "lemma_list = [lemmatizer.lemmatize(word, pos='n') for word in word_list]\n",
    " \n",
    "print(\"Original words:\", word_list)\n",
    "print(\"Lemmas:\", lemma_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d064a94a-7721-4103-8d2b-38c6a1a8195a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['runnable', 'walkable', 'changable']\n",
      "Lemmas: ['runnable', 'walkable', 'changable']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization example\n",
    "word_list =  ['runnable', 'walkable', 'changable']\n",
    "lemma_list = [lemmatizer.lemmatize(word,pos='a') for word in word_list]\n",
    " \n",
    "print(\"Original words:\", word_list)\n",
    "print(\"Lemmas:\", lemma_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d1453e8-4903-4365-9652-7331f294d8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['better', 'happily', 'happy']\n",
      "Lemmas: ['good', 'happily', 'happy']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization example\n",
    "word_list = ['better', 'happily', 'happy']\n",
    "lemma_list = [lemmatizer.lemmatize(word,pos='a') for word in word_list]\n",
    " \n",
    "print(\"Original words:\", word_list)\n",
    "print(\"Lemmas:\", lemma_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "829d7dff-d64c-4a2b-a6be-6e25c3155702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['better']\n",
      "Lemmas: ['well']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization example\n",
    "word_list = ['better']\n",
    "lemma_list = [lemmatizer.lemmatize(word,pos='r') for word in word_list]\n",
    " \n",
    "print(\"Original words:\", word_list)\n",
    "print(\"Lemmas:\", lemma_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3475957-b84b-4dd8-a263-14bfc0fa0b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['Hello', 'everyone', 'Today', 'sunny', 'day']\n",
      "Lemmas: ['Hello', 'everyone', 'Today', 'sunny', 'day']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization example\n",
    "word_list = word_tokens\n",
    "word_token_lemma_list = [lemmatizer.lemmatize(word) for word in word_list]\n",
    " \n",
    "print(\"Original words:\", word_list)\n",
    "print(\"Lemmas:\", word_token_lemma_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438af281-bf8f-4c9f-871b-3a0a67a8b2ee",
   "metadata": {},
   "source": [
    "### Part of Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95694df9-cf28-4bdb-8077-1c8fa1d65cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\nadda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For POS \n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf9930e5-609c-43c3-84a7-8f4fe36c4f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['Hello', 'everyone', 'Today', 'sunny', 'day']\n",
      "POS tagged: [('Hello', 'NNP'), ('everyone', 'NN'), ('Today', 'NNP'), ('sunny', 'VBD'), ('day', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# list of pos tags: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "# Note: 'NN' = noun, 'NNP' = proper noun, 'VBD' = past-tense verb\n",
    "\n",
    "from nltk import pos_tag\n",
    "pos_tags = pos_tag(word_tokens)\n",
    "print(\"Original words:\", word_tokens)\n",
    "print(\"POS tagged:\", pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3db31813-84af-43ee-a92e-6226890a05bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['Hello', 'everyone', 'Today', 'sunny', 'day']\n",
      "Lemmas without pos: ['Hello', 'everyone', 'Today', 'sunny', 'day']\n",
      "Lemmas: ['Hello', 'everyone', 'Today', 'sunny', 'day']\n"
     ]
    }
   ],
   "source": [
    "# To obtain POS before running lemmatization \n",
    "# When lemmatization is executed, can use POS as value of pos parameter\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag_dict = {'N': wordnet.NOUN,\n",
    "                'V': wordnet.VERB,\n",
    "                'J': wordnet.ADJ,\n",
    "                'R': wordnet.ADV}    \n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper() # Get first character of pos tag from word\n",
    "    return tag_dict.get(tag, wordnet.NOUN)  # default when not found = 'N'\n",
    "    \n",
    "# Lemmatization\n",
    "word_list = word_tokens\n",
    "lemma_list = [lemmatizer.lemmatize(word, pos=get_wordnet_pos(word)) for word in word_list]\n",
    " \n",
    "print(\"Original words:\", word_list)\n",
    "print(\"Lemmas without pos:\", word_token_lemma_list)\n",
    "print(\"Lemmas:\", lemma_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7ecc81-f23e-44a7-8c3f-cd191e455682",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1517aa90-cbf6-4e30-b7d2-15fa47c20e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "corpus_stop_words = set(stopwords.words('english')) \n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = [word.lower() for word in word_tokenize(text)] \n",
    "    tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokens if re.sub(r'[^\\w\\s]', '', token)]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    tokens = [word for word in tokens if not word in corpus_stop_words]  \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d04864c9-ade0-46e1-bd40-1eb92d2ba5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = 'A cat sits nearby the window'\n",
    "second = 'A dog sits nearby the window'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7732e8c5-8e40-4fbf-98b7-945abd6b5b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sits', 'dog', 'cat', 'nearby', 'window']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_set = set(preprocess_text(first))\n",
    "second_set = set(preprocess_text(second))\n",
    "all_tokens = list(first_set.union(second_set))\n",
    "all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4600fd0e-6c96-4976-ac3a-4b0987d0bd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat', 'sits', 'nearby', 'window'}\n",
      "{'sits', 'window', 'nearby', 'dog'}\n"
     ]
    }
   ],
   "source": [
    "print(first_set)\n",
    "print(second_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f6f6b78e-072e-418e-b87a-aa2dba8d33c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 1, 1]\n",
      "[1, 1, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "first_exist = [1 if word in first_set else 0 for word in all_tokens]\n",
    "second_exist = [1 if word in second_set else 0 for word in all_tokens]\n",
    "print(first_exist)\n",
    "print(second_exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c3832cf7-c86c-4f97-a102-0a1829d18de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squared_first_sum = sum([val*val for val in first_exist])**0.5\n",
    "squared_second_sum = sum([val*val for val in second_exist])**0.5\n",
    "cosine_similar = np.dot(first_exist, second_exist) / (squared_first_sum*squared_second_sum)\n",
    "cosine_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a049487d-bb6c-47ec-a0c4-56a3907c383b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_dist = 1 - cosine_similar\n",
    "cosine_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e9c797-f2d8-448d-a5a1-2b19c6bf8cb8",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "316b2a67-2736-4edf-96ad-17580ac5347b",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = 'It is going to rain today.' \n",
    "second = 'Today I am not going outside.' \n",
    "third = 'I am going to watch TV.' \n",
    "texts = [first, second, third]\n",
    "texts = [text.lower() for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7dff3815-d623-40aa-bf39-6dcd6b5a8984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word indexes: {'it': 3, 'is': 2, 'going': 1, 'to': 7, 'rain': 6, 'today': 8, 'am': 0, 'not': 4, 'outside': 5, 'watch': 10, 'tv': 9}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "result = tfidf.fit_transform(texts)\n",
    "print('Word indexes:', tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "20095dc8-07f9-4637-861d-ebb9f7a1b517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf value:\n",
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 16 stored elements and shape (3, 11)>\n",
      "  Coords\tValues\n",
      "  (0, 3)\t0.4711101009983051\n",
      "  (0, 2)\t0.4711101009983051\n",
      "  (0, 1)\t0.2782452148327134\n",
      "  (0, 7)\t0.35829137488557944\n",
      "  (0, 6)\t0.4711101009983051\n",
      "  (0, 8)\t0.35829137488557944\n",
      "  (1, 1)\t0.3154441510317797\n",
      "  (1, 8)\t0.4061917781433946\n",
      "  (1, 0)\t0.4061917781433946\n",
      "  (1, 4)\t0.5340933749435833\n",
      "  (1, 5)\t0.5340933749435833\n",
      "  (2, 1)\t0.3154441510317797\n",
      "  (2, 7)\t0.4061917781433946\n",
      "  (2, 0)\t0.4061917781433946\n",
      "  (2, 10)\t0.5340933749435833\n",
      "  (2, 9)\t0.5340933749435833\n"
     ]
    }
   ],
   "source": [
    "print('tf-idf value:')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f2e866e-bcd0-45b2-b07d-dd7dd04f636c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf values in matrix form:\n",
      "[[0.         0.27824521 0.4711101  0.4711101  0.         0.\n",
      "  0.4711101  0.35829137 0.35829137 0.         0.        ]\n",
      " [0.40619178 0.31544415 0.         0.         0.53409337 0.53409337\n",
      "  0.         0.         0.40619178 0.         0.        ]\n",
      " [0.40619178 0.31544415 0.         0.         0.         0.\n",
      "  0.         0.40619178 0.         0.53409337 0.53409337]]\n"
     ]
    }
   ],
   "source": [
    "print('tf-idf values in matrix form:')\n",
    "print(result.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dca5fff7-179a-4b04-9b67-f7e39c643004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am : 1.2876820724517808\n",
      "going : 1.0\n",
      "is : 1.6931471805599454\n",
      "it : 1.6931471805599454\n",
      "not : 1.6931471805599454\n",
      "outside : 1.6931471805599454\n",
      "rain : 1.6931471805599454\n",
      "to : 1.2876820724517808\n",
      "today : 1.2876820724517808\n",
      "tv : 1.6931471805599454\n",
      "watch : 1.6931471805599454\n"
     ]
    }
   ],
   "source": [
    "for word, idf in zip(tfidf.get_feature_names_out(), tfidf.idf_):\n",
    "    print(word, ':', idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ffef747e-d51c-40b7-96ba-4778c2fd4e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf values in matrix form:\n",
      "[[0.         0.27824521 0.4711101  0.4711101  0.         0.\n",
      "  0.4711101  0.35829137 0.35829137 0.         0.        ]\n",
      " [0.40619178 0.31544415 0.         0.         0.53409337 0.53409337\n",
      "  0.         0.         0.40619178 0.         0.        ]\n",
      " [0.40619178 0.31544415 0.         0.         0.         0.\n",
      "  0.         0.40619178 0.         0.53409337 0.53409337]]\n"
     ]
    }
   ],
   "source": [
    "print('tf-idf values in matrix form:')\n",
    "print(result.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b0f2eb-ff79-4c44-aa51-ae73182b2470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a14f83-e241-4ea0-a217-2a6e30f09b67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
