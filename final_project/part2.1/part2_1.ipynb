{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data and Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nadda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nadda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\nadda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nadda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\nadda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r'C:\\Users\\nadda\\Desktop\\KU\\01204314-65-Statistics-for-Computer-Engineering-Applications\\final_project\\part2.1')\n",
    "os.getcwd()\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # optional แต่ช่วยให้ lemmatizer ทำงานดีขึ้น\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"africa.txt\", \"r\", encoding='utf-8')\n",
    "africa = file.read()\n",
    "file.close()\n",
    "\n",
    "file = open(\"asia.txt\", \"r\", encoding='utf-8')\n",
    "asia = file.read()\n",
    "file.close()\n",
    "\n",
    "file = open(\"europe.txt\", \"r\", encoding='utf-8')\n",
    "europe = file.read()\n",
    "file.close()\n",
    "\n",
    "file = open(\"latin_america.txt\", \"r\", encoding='utf-8')\n",
    "latin_america = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "africa_token = word_tokenize(africa)\n",
    "asia_token = word_tokenize(asia)\n",
    "europe_token = word_tokenize(europe)\n",
    "latin_america_token = word_tokenize(latin_america)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "africa_token = [re.sub(r'[^\\w\\s]', '', token) for token in africa_token if re.sub(r'[^\\w\\s]', '', token)]\n",
    "asia_token = [re.sub(r'[^\\w\\s]', '', token) for token in asia_token if re.sub(r'[^\\w\\s]', '', token)]\n",
    "europe_token = [re.sub(r'[^\\w\\s]', '', token) for token in europe_token if re.sub(r'[^\\w\\s]', '', token)]\n",
    "latin_america_token = [re.sub(r'[^\\w\\s]', '', token) for token in latin_america_token if re.sub(r'[^\\w\\s]', '', token)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "africa_token = [token for token in africa_token if token.lower() not in stop_words]\n",
    "asia_token = [token for token in asia_token if token.lower() not in stop_words]\n",
    "europe_token = [token for token in europe_token if token.lower() not in stop_words]\n",
    "latin_america_token = [token for token in latin_america_token if token.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tagging\n",
    "africa_pos = pos_tag(africa_token)\n",
    "asia_pos = pos_tag(asia_token)\n",
    "europe_pos = pos_tag(europe_token)\n",
    "latin_america_pos = pos_tag(latin_america_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# แปลง POS tag จาก nltk → wordnet format\n",
    "def convert_pos(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # default เป็นคำนาม\n",
    "    \n",
    "def lemmatize_and_lowercase(word_pos_list):\n",
    "    result = []\n",
    "    for word, pos in word_pos_list:\n",
    "        wn_pos = convert_pos(pos)\n",
    "        lemma = lemmatizer.lemmatize(word, wn_pos)\n",
    "        if wn_pos != wordnet.NOUN:\n",
    "            lemma = lemma.lower()\n",
    "        result.append(lemma)\n",
    "    return result\n",
    "\n",
    "africa_lemmatized = lemmatize_and_lowercase(africa_pos)\n",
    "asia_lemmatized = lemmatize_and_lowercase(asia_pos)\n",
    "europe_lemmatized = lemmatize_and_lowercase(europe_pos)\n",
    "latin_america_lemmatized = lemmatize_and_lowercase(latin_america_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 Most related Words to \"gender\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('equality', 0.7441256046295166),\n",
       " ('reach', 0.7101626396179199),\n",
       " ('improvement', 0.6999989151954651),\n",
       " ('room', 0.6813947558403015),\n",
       " ('still', 0.681369960308075),\n",
       " ('vary', 0.6641895771026611),\n",
       " ('encourage', 0.6611723303794861),\n",
       " ('gaps', 0.6609336137771606),\n",
       " ('basis', 0.6480785608291626),\n",
       " ('adopt', 0.6382238864898682),\n",
       " ('much', 0.6311314105987549),\n",
       " ('full', 0.6296327710151672),\n",
       " ('commitment', 0.6286253333091736),\n",
       " ('towards', 0.6273753643035889),\n",
       " ('fact', 0.624669075012207),\n",
       " ('Despite', 0.6235596537590027),\n",
       " ('analyze', 0.6224499940872192),\n",
       " ('persist', 0.6219894289970398),\n",
       " ('trend', 0.6170200705528259),\n",
       " ('inequality', 0.6161401867866516),\n",
       " ('underestimate', 0.6156435012817383),\n",
       " ('widely', 0.6152939796447754),\n",
       " ('member', 0.6131133437156677),\n",
       " ('Data', 0.6126700639724731),\n",
       " ('substantive', 0.6092368364334106),\n",
       " ('dimension', 0.607306957244873),\n",
       " ('Reduce', 0.606299638748169),\n",
       " ('association', 0.6027237772941589),\n",
       " ('expansion', 0.5994047522544861),\n",
       " ('priority', 0.5961561799049377)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# รวมทุก region: [[word1, word2, ...], [word1, word2, ...], ...]\n",
    "all_tokens = [africa_lemmatized, asia_lemmatized, europe_lemmatized, latin_america_lemmatized]\n",
    "\n",
    "# Train Word2Vec\n",
    "model = Word2Vec(\n",
    "    sentences=all_tokens,\n",
    "    vector_size=100,   # ขนาดเวกเตอร์\n",
    "    window=5,          # ขอบเขตคำรอบข้าง\n",
    "    min_count=1,       # เอาทุกคำ แม้เจอน้อย\n",
    "    sg=1,              # ใช้ Skip-gram\n",
    "    epochs=100,        # รอบการ train\n",
    "    seed=42            # reproducibility\n",
    ")\n",
    "\n",
    "# หา 30 คำที่ใกล้คำว่า \"gender\"\n",
    "print('30 Most related Words to \"gender\"')\n",
    "model.wv.most_similar('gender', topn=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 Most related Words to \"equality\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('gaps', 0.7567237019538879),\n",
       " ('gender', 0.7441256642341614),\n",
       " ('substantive', 0.7253093123435974),\n",
       " ('Data', 0.69869065284729),\n",
       " ('commitment', 0.6740468144416809),\n",
       " ('towards', 0.6629829406738281),\n",
       " ('reach', 0.6616057753562927),\n",
       " ('dimension', 0.6502516269683838),\n",
       " ('adopt', 0.6367909908294678),\n",
       " ('improvement', 0.635064423084259),\n",
       " ('Incentives', 0.6318537592887878),\n",
       " ('analyze', 0.6291241645812988),\n",
       " ('full', 0.6269788146018982),\n",
       " ('Reduce', 0.6158241629600525),\n",
       " ('commit', 0.610556423664093),\n",
       " ('conduct', 0.6088600158691406),\n",
       " ('entity', 0.6078664660453796),\n",
       " ('various', 0.6077552437782288),\n",
       " ('adoption', 0.6054598689079285),\n",
       " ('room', 0.6028990149497986),\n",
       " ('framework', 0.5994222164154053),\n",
       " ('encourage', 0.5982713103294373),\n",
       " ('however', 0.5981284976005554),\n",
       " ('mainstreaming', 0.5964041948318481),\n",
       " ('member', 0.5925891995429993),\n",
       " ('expansion', 0.5868939757347107),\n",
       " ('governance', 0.5859586596488953),\n",
       " ('empower', 0.5858578681945801),\n",
       " ('code', 0.5839506983757019),\n",
       " ('still', 0.5836881995201111)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Word2Vec\n",
    "model = Word2Vec(\n",
    "    sentences=all_tokens,\n",
    "    vector_size=100,   # ขนาดเวกเตอร์\n",
    "    window=5,          # ขอบเขตคำรอบข้าง\n",
    "    min_count=1,       # เอาทุกคำ แม้เจอน้อย\n",
    "    sg=1,              # ใช้ Skip-gram\n",
    "    epochs=100,        # รอบการ train\n",
    "    seed=42            # reproducibility\n",
    ")\n",
    "\n",
    "# หา 30 คำที่ใกล้คำว่า \"equality\"\n",
    "print('30 Most related Words to \"equality\"')\n",
    "model.wv.most_similar('equality', topn=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
