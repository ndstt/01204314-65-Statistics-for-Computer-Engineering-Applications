{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data and Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nadda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nadda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\nadda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nadda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\nadda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r'C:\\Users\\nadda\\Desktop\\KU\\01204314-65-Statistics-for-Computer-Engineering-Applications\\final_project\\part2.1')\n",
    "os.getcwd()\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # optional à¹à¸•à¹ˆà¸Šà¹ˆà¸§à¸¢à¹ƒà¸«à¹‰ lemmatizer à¸—à¸³à¸‡à¸²à¸™à¸”à¸µà¸‚à¸¶à¹‰à¸™\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"africa.txt\", \"r\", encoding='utf-8')\n",
    "africa = file.read()\n",
    "file.close()\n",
    "\n",
    "file = open(\"asia.txt\", \"r\", encoding='utf-8')\n",
    "asia = file.read()\n",
    "file.close()\n",
    "\n",
    "file = open(\"europe.txt\", \"r\", encoding='utf-8')\n",
    "europe = file.read()\n",
    "file.close()\n",
    "\n",
    "file = open(\"latin_america.txt\", \"r\", encoding='utf-8')\n",
    "latin_america = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "africa_token = word_tokenize(africa)\n",
    "asia_token = word_tokenize(asia)\n",
    "europe_token = word_tokenize(europe)\n",
    "latin_america_token = word_tokenize(latin_america)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "africa_token = [re.sub(r'[^\\w\\s]', '', token) for token in africa_token if re.sub(r'[^\\w\\s]', '', token)]\n",
    "asia_token = [re.sub(r'[^\\w\\s]', '', token) for token in asia_token if re.sub(r'[^\\w\\s]', '', token)]\n",
    "europe_token = [re.sub(r'[^\\w\\s]', '', token) for token in europe_token if re.sub(r'[^\\w\\s]', '', token)]\n",
    "latin_america_token = [re.sub(r'[^\\w\\s]', '', token) for token in latin_america_token if re.sub(r'[^\\w\\s]', '', token)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "africa_token = [token for token in africa_token if token.lower() not in stop_words]\n",
    "asia_token = [token for token in asia_token if token.lower() not in stop_words]\n",
    "europe_token = [token for token in europe_token if token.lower() not in stop_words]\n",
    "latin_america_token = [token for token in latin_america_token if token.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tagging\n",
    "africa_pos = pos_tag(africa_token)\n",
    "asia_pos = pos_tag(asia_token)\n",
    "europe_pos = pos_tag(europe_token)\n",
    "latin_america_pos = pos_tag(latin_america_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# à¹à¸›à¸¥à¸‡ POS tag à¸ˆà¸²à¸ nltk â†’ wordnet format\n",
    "def convert_pos(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # default à¹€à¸›à¹‡à¸™à¸„à¸³à¸™à¸²à¸¡\n",
    "    \n",
    "def lemmatize_and_lowercase(word_pos_list):\n",
    "    result = []\n",
    "    for word, pos in word_pos_list:\n",
    "        wn_pos = convert_pos(pos)\n",
    "        lemma = lemmatizer.lemmatize(word, wn_pos)\n",
    "        if wn_pos != wordnet.NOUN:\n",
    "            lemma = lemma.lower()\n",
    "        result.append(lemma)\n",
    "    return result\n",
    "\n",
    "africa_lemmatized = lemmatize_and_lowercase(africa_pos)\n",
    "asia_lemmatized = lemmatize_and_lowercase(asia_pos)\n",
    "europe_lemmatized = lemmatize_and_lowercase(europe_pos)\n",
    "latin_america_lemmatized = lemmatize_and_lowercase(latin_america_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "region_names = ['africa', 'asia', 'europe', 'latin_america']\n",
    "region_tokens = [africa_lemmatized, asia_lemmatized, europe_lemmatized, latin_america_lemmatized]\n",
    "\n",
    "region_models = {}\n",
    "\n",
    "for name, tokens in zip(region_names, region_tokens):\n",
    "    model = Word2Vec(\n",
    "        sentences=[tokens],     # à¸•à¹‰à¸­à¸‡à¹€à¸›à¹‡à¸™ list of list\n",
    "        vector_size=100,\n",
    "        window=5,\n",
    "        min_count=1,\n",
    "        sg=1,\n",
    "        epochs=100,\n",
    "        seed=42\n",
    "    )\n",
    "    region_models[name] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['gender', 'equality']\n",
    "related_words = {}\n",
    "\n",
    "for name, model in region_models.items():\n",
    "    related_words[name] = {}\n",
    "    for key in keywords:\n",
    "        if key in model.wv:\n",
    "            related = model.wv.most_similar(key, topn=30)\n",
    "            related_words[name][key] = [word for word, score in related]\n",
    "        else:\n",
    "            related_words[name][key] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ AFRICA â€” Related terms\n",
      "  gender: ['equality', 'towards', 'continue', 'economic', 'progress', 'Reduce', 'sexsegregation', 'measure', 'gaps', 'empower', 'labour', 'take', 'market', 'elimination', 'discrimination', 'Data', 'make', 'adopt', 'priority', 'conduct', 'member', 'lens', 'encourage', 'several', 'suggest', 'hidden', 'association', 'Given', 'code', 'law']\n",
      "  equality: ['continue', 'gender', 'towards', 'progress', 'economic', 'gaps', 'empower', 'Reduce', 'Data', 'sexsegregation', 'take', 'labour', 'market', 'measure', 'several', 'national', 'elimination', 'discrimination', 'priority', 'suggest', 'adopt', 'hidden', 'law', 'institute', 'pose', 'report', 'bias', 'member', 'Given', 'make']\n",
      "\n",
      "ðŸ”¹ ASIA â€” Related terms\n",
      "  gender: ['equality', 'commitment', 'across', 'awareness', 'expansion', 'mainstreaming', 'sector', 'entity', '19', 'genderresponsive', 'budgeting', 'machineries', 'c', 'governance', 'achieve', 'concerning', 'institutional', 'adoption', 'term', 'promote', 'violence', 'girls20', 'framework', 'government', 'strengthen', 'equalityrelated', 'enforcement', 'law', 'mandate', 'legislation']\n",
      "  equality: ['gender', 'commitment', 'awareness', 'across', '19', 'expansion', 'mainstreaming', 'governance', 'institutional', 'sector', 'promote', 'term', 'achieve', 'c', 'machineries', 'budgeting', 'entity', 'framework', 'genderresponsive', 'violence', 'strengthen', 'concerning', 'girls20', 'adoption', 'law', 'enforcement', 'national', 'capacity', 'b', 'mandate']\n",
      "\n",
      "ðŸ”¹ EUROPE â€” Related terms\n",
      "  gender: ['gap', 'equality', 'still', 'show', 'level', 'rate', 'towards', 'basis', 'Given', 'may', 'full', 'persist', 'equivalent', 'parttime', 'men', 'woman', 'Incentives', 'Full', 'employment', 'even', 'underestimate', 'equivalence', 'increase', 'fact', 'reach', 'without', 'encourage', 'comparison', 'resort', 'heart']\n",
      "  equality: ['gender', 'towards', 'EU', 'Incentives', 'still', 'show', 'level', 'encourage', 'gap', 'increase', 'reach', 'men', 'remain', 'woman', 'improvement', 'Despite', 'room', 'heart', 'parttime', 'Given', 'life', 'much', 'rate', 'full', 'increasingly', 'agenda', 'trend', 'basis', 'even', 'persist']\n",
      "\n",
      "ðŸ”¹ LATIN_AMERICA â€” Related terms\n",
      "  gender: ['inequality', 'public', 'floor', 'sticky', 'comprehensively', 'analyze', 'lack', 'deepen', 'various', 'dimension', 'socalled', 'persistence', 'household', 'achieve', 'address', 'worklife', 'especially', 'contribute', 'maternity', 'balance', 'related', 'particular', 'substantive', 'move', 'situation', 'structural', 'market', 'towards', 'penalization', 'inclusive']\n",
      "  equality: ['commit', 'however', 'right', 'fundamental', 'promote', 'human', 'substantive', 'achieve', 'order', 'dimension', 'adequate', 'comprehensive', 'policy', 'implies', 'implement', 'invest', 'recognize', 'sustainable', 'system', 'equitable', 'make', 'comprehensively', 'wellbeing', 'address', 'analyze', 'crucial', 'Women', 'various', 'line', 'need']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# à¸£à¸§à¸¡à¸„à¸³à¸—à¸µà¹ˆà¹€à¸à¸µà¹ˆà¸¢à¸§à¸‚à¹‰à¸­à¸‡à¸—à¸¸à¸à¸„à¸³à¹ƒà¸™à¹à¸•à¹ˆà¸¥à¸° region\n",
    "for region in region_names:\n",
    "    print(f\"ðŸ”¹ {region.upper()} â€” Related terms\")\n",
    "    for key in keywords:\n",
    "        print(f\"  {key}: {related_words[region][key]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== ðŸ” Comparing related words for: 'gender' ====\n",
      "âœ… Common across regions: ['equality', 'towards', 'market', 'encourage', 'Given', 'law', 'achieve']\n",
      "\n",
      "==== ðŸ” Comparing related words for: 'equality' ====\n",
      "âœ… Common across regions: ['gender', 'towards', 'national', 'law', 'Given', 'make', 'promote', 'achieve']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for key in keywords:\n",
    "    print(f\"==== ðŸ” Comparing related words for: '{key}' ====\")\n",
    "    # à¸£à¸§à¸¡à¸—à¸¸à¸à¸„à¸³à¸‚à¸­à¸‡à¸„à¸³à¸«à¸¥à¸±à¸ key à¸ˆà¸²à¸à¸—à¸¸à¸ region\n",
    "    all_words = sum([related_words[r][key] for r in region_names], [])\n",
    "    word_counts = Counter(all_words)\n",
    "    print(\"âœ… Common across regions:\", [w for w, c in word_counts.items() if c > 1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== ðŸ” Unique related words for: 'gender' ====\n",
      "\n",
      "ðŸŸ¦ AFRICA (24 unique terms)\n",
      "['continue', 'economic', 'progress', 'Reduce', 'sexsegregation', 'measure', 'gaps', 'empower', 'labour', 'take', 'elimination', 'discrimination', 'Data', 'make', 'adopt', 'priority', 'conduct', 'member', 'lens', 'several', 'suggest', 'hidden', 'association', 'code']\n",
      "\n",
      "ðŸŸ¦ ASIA (27 unique terms)\n",
      "['commitment', 'across', 'awareness', 'expansion', 'mainstreaming', 'sector', 'entity', '19', 'genderresponsive', 'budgeting', 'machineries', 'c', 'governance', 'concerning', 'institutional', 'adoption', 'term', 'promote', 'violence', 'girls20', 'framework', 'government', 'strengthen', 'equalityrelated', 'enforcement', 'mandate', 'legislation']\n",
      "\n",
      "ðŸŸ¦ EUROPE (26 unique terms)\n",
      "['gap', 'still', 'show', 'level', 'rate', 'basis', 'may', 'full', 'persist', 'equivalent', 'parttime', 'men', 'woman', 'Incentives', 'Full', 'employment', 'even', 'underestimate', 'equivalence', 'increase', 'fact', 'reach', 'without', 'comparison', 'resort', 'heart']\n",
      "\n",
      "ðŸŸ¦ LATIN_AMERICA (27 unique terms)\n",
      "['inequality', 'public', 'floor', 'sticky', 'comprehensively', 'analyze', 'lack', 'deepen', 'various', 'dimension', 'socalled', 'persistence', 'household', 'address', 'worklife', 'especially', 'contribute', 'maternity', 'balance', 'related', 'particular', 'substantive', 'move', 'situation', 'structural', 'penalization', 'inclusive']\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "\n",
      "==== ðŸ” Unique related words for: 'equality' ====\n",
      "\n",
      "ðŸŸ¦ AFRICA (24 unique terms)\n",
      "['continue', 'progress', 'economic', 'gaps', 'empower', 'Reduce', 'Data', 'sexsegregation', 'take', 'labour', 'market', 'measure', 'several', 'elimination', 'discrimination', 'priority', 'suggest', 'adopt', 'hidden', 'institute', 'pose', 'report', 'bias', 'member']\n",
      "\n",
      "ðŸŸ¦ ASIA (25 unique terms)\n",
      "['commitment', 'awareness', 'across', '19', 'expansion', 'mainstreaming', 'governance', 'institutional', 'sector', 'term', 'c', 'machineries', 'budgeting', 'entity', 'framework', 'genderresponsive', 'violence', 'strengthen', 'concerning', 'girls20', 'adoption', 'enforcement', 'capacity', 'b', 'mandate']\n",
      "\n",
      "ðŸŸ¦ EUROPE (27 unique terms)\n",
      "['EU', 'Incentives', 'still', 'show', 'level', 'encourage', 'gap', 'increase', 'reach', 'men', 'remain', 'woman', 'improvement', 'Despite', 'room', 'heart', 'parttime', 'life', 'much', 'rate', 'full', 'increasingly', 'agenda', 'trend', 'basis', 'even', 'persist']\n",
      "\n",
      "ðŸŸ¦ LATIN_AMERICA (27 unique terms)\n",
      "['commit', 'however', 'right', 'fundamental', 'human', 'substantive', 'order', 'dimension', 'adequate', 'comprehensive', 'policy', 'implies', 'implement', 'invest', 'recognize', 'sustainable', 'system', 'equitable', 'comprehensively', 'wellbeing', 'address', 'analyze', 'crucial', 'Women', 'various', 'line', 'need']\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "for key in keywords:\n",
    "    print(f\"==== ðŸ” Unique related words for: '{key}' ====\")\n",
    "\n",
    "    # à¹€à¸•à¸£à¸µà¸¢à¸¡ map: word -> regions à¸—à¸µà¹ˆà¸¡à¸µà¸„à¸³à¸™à¸±à¹‰à¸™\n",
    "    word_to_regions = defaultdict(list)\n",
    "    for region in region_names:\n",
    "        for word in related_words[region][key]:\n",
    "            word_to_regions[word].append(region)\n",
    "\n",
    "    # à¸”à¸¶à¸‡à¹€à¸‰à¸žà¸²à¸°à¸„à¸³à¸—à¸µà¹ˆà¸­à¸¢à¸¹à¹ˆà¹ƒà¸™ region à¹€à¸”à¸µà¸¢à¸§à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™\n",
    "    for region in region_names:\n",
    "        unique_words = [\n",
    "            word for word in related_words[region][key]\n",
    "            if word_to_regions[word] == [region]\n",
    "        ]\n",
    "        print(f\"\\nðŸŸ¦ {region.upper()} ({len(unique_words)} unique terms)\")\n",
    "        print(unique_words)\n",
    "    print(\"________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
